{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python pytesseract tensorflow==2.12.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_Ux-Vpcoy5GM",
        "outputId": "3f6362cd-2218-4145-f229-cf475668b606"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting tensorflow==2.12.0\n",
            "  Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (25.2.10)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.12.0)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.73.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.14.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.5.2)\n",
            "Collecting keras<2.13,>=2.12.0 (from tensorflow==2.12.0)\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (18.1.1)\n",
            "Collecting numpy<1.24,>=1.22 (from tensorflow==2.12.0)\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (25.0)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.12.0)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.17.0)\n",
            "Collecting tensorboard<2.13,>=2.12 (from tensorflow==2.12.0)\n",
            "  Downloading tensorboard-2.12.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow==2.12.0)\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.14.1)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.12.0)\n",
            "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.37.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.2.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.5.2,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.5.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.1)\n",
            "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.6.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib<=0.6.2,>=0.6.2 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.6.2-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting ml_dtypes>=0.5.0 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.6.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib<=0.6.1,>=0.6.1 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.6.1-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.6.0,>=0.6.0 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.6.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.5.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.3,>=0.5.3 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.5.3-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.5.1-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.0,>=0.5.0 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.5.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.38-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.38,>=0.4.38 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.38-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "INFO: pip is still looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.37-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.37,>=0.4.36 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.36-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.35-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.34,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.34-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.33-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.33,>=0.4.33 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.33-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.31,>=0.4.30 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.31-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.30,>=0.4.27 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.15.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.38.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow==2.12.0)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.3.1)\n",
            "Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (586.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.0/586.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m122.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl (79.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, pytesseract, protobuf, numpy, keras, gast, jaxlib, google-auth-oauthlib, tensorboard, jax, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.8.0\n",
            "    Uninstalling keras-3.8.0:\n",
            "      Successfully uninstalled keras-3.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.5.1\n",
            "    Uninstalling jaxlib-0.5.1:\n",
            "      Successfully uninstalled jaxlib-0.5.1\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.2\n",
            "    Uninstalling google-auth-oauthlib-1.2.2:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.2\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.5.2\n",
            "    Uninstalling jax-0.5.2:\n",
            "      Successfully uninstalled jax-0.5.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray-einstats 0.9.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.24.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "orbax-checkpoint 0.11.16 requires jax>=0.5.0, but you have jax 0.4.30 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.23.5 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.12.0 which is incompatible.\n",
            "flax 0.10.6 requires jax>=0.5.1, but you have jax 0.4.30 which is incompatible.\n",
            "blosc2 3.6.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "db-dtypes 1.4.3 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "bigframes 2.11.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-1.0.0 jax-0.4.30 jaxlib-0.4.30 keras-2.12.0 numpy-1.23.5 protobuf-4.25.8 pytesseract-0.3.13 tensorboard-2.12.3 tensorflow-2.12.0 tensorflow-estimator-2.12.0 wrapt-1.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy"
                ]
              },
              "id": "343a3ee22a9f4529bb554a99c6630460"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pytesseract\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # This is the correct way to mount your google drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7qbT8JVy5Sd",
        "outputId": "810bcc7a-dd49-453e-9edb-80059146c975"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://pjreddie.com/media/files/yolov3.weights  # Download weights\n",
        "!wget https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg  # Download configuration"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i_fniTgy5VF",
        "outputId": "c6c3ff8d-579b-4b47-8e95-111c384b1d3d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-22 11:05:43--  https://pjreddie.com/media/files/yolov3.weights\n",
            "Resolving pjreddie.com (pjreddie.com)... 104.21.88.156, 172.67.185.199, 2606:4700:3037::6815:589c, ...\n",
            "Connecting to pjreddie.com (pjreddie.com)|104.21.88.156|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘yolov3.weights’\n",
            "\n",
            "\ryolov3.weights          [<=>                 ]       0  --.-KB/s               \ryolov3.weights          [ <=>                ]   8.88K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-07-22 11:05:43 (83.8 MB/s) - ‘yolov3.weights’ saved [9093]\n",
            "\n",
            "--2025-07-22 11:05:43--  https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8342 (8.1K) [text/plain]\n",
            "Saving to: ‘yolov3.cfg’\n",
            "\n",
            "yolov3.cfg          100%[===================>]   8.15K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-07-22 11:05:43 (85.8 MB/s) - ‘yolov3.cfg’ saved [8342/8342]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uahgZybmy5Yb",
        "outputId": "cc2de5fa-7ddc-4be9-fe4b-a24e87592172"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_images(image_folder, target_size=(608, 608), max_images=100):\n",
        "    image_files = [f for f in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, f))]\n",
        "    image_files = image_files[:max_images]  # Limit to max_images\n",
        "\n",
        "    for filename in image_files:\n",
        "        if filename.endswith(('.jpg', '.png', '.jpeg')):  # Adjust file extensions as needed\n",
        "            image_path = os.path.join(image_folder, filename)\n",
        "            img = cv2.imread(image_path)\n",
        "            resized_img = cv2.resize(img, target_size)\n",
        "            cv2.imwrite(image_path, resized_img)  # Overwrite original with resized image\n",
        "\n",
        "# Set paths\n",
        "dataset_folder = '/content/drive/MyDrive/Dataset_OCR/'  # Path to your Dataset_OCR folder\n",
        "image_folder = dataset_folder  # Assuming images are directly in Dataset_OCR\n",
        "\n",
        "# Resize images\n",
        "resize_images(image_folder)\n",
        "\n",
        "print(f\"Resized {len(os.listdir(image_folder))} images in {image_folder}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAAuxvyYy5bn",
        "outputId": "c999531f-ddaa-4a87-a4f8-b5efca9a9ab3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resized 11 images in /content/drive/MyDrive/Dataset_OCR/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Task 2.2**\n",
        "\n",
        " **Prepare the Dataset**\n",
        "\n",
        "○ Download and upload the dataset to Google Drive. -  \n",
        "○ Preprocess dataset: resizing images, creating YOLO annotations. -  \n",
        "○ Store preprocessed data in Drive"
      ],
      "metadata": {
        "id": "KcdGSLPZ2Eij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save the gray size resized image in a folder with Name Gray Size in the same drive**"
      ],
      "metadata": {
        "id": "gYkN8Df02PKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "def resize_and_save_gray(image_folder, output_folder, target_size=(608, 608), max_images=100):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    image_files = [f for f in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, f))]\n",
        "    image_files = image_files[:max_images]\n",
        "\n",
        "    for filename in image_files:\n",
        "        if filename.endswith(('.jpg', '.png', '.jpeg')):\n",
        "            image_path = os.path.join(image_folder, filename)\n",
        "            img = cv2.imread(image_path)\n",
        "\n",
        "            # Convert to grayscale\n",
        "            gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # Resize the grayscale image\n",
        "            resized_gray_img = cv2.resize(gray_img, target_size)\n",
        "\n",
        "            # Construct the output path\n",
        "            output_path = os.path.join(output_folder, filename)\n",
        "\n",
        "            # Save the resized grayscale image\n",
        "            cv2.imwrite(output_path, resized_gray_img)\n",
        "\n",
        "# Example usage\n",
        "dataset_folder = '/content/drive/MyDrive/Dataset_OCR/'\n",
        "output_folder = '/content/drive/MyDrive/Gray_Size' # Output folder\n",
        "\n",
        "resize_and_save_gray(dataset_folder, output_folder)\n",
        "\n",
        "print(f\"Resized grayscale images saved to: {output_folder}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsLGREuBy5e_",
        "outputId": "c2b68bb6-904b-4689-9107-3eeff7d36c4d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resized grayscale images saved to: /content/drive/MyDrive/Gray_Size\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Model Training**\n",
        "# ● Task 3.1: Train YOLOv3 Model\n",
        "# ○ Train YOLOv3 model using Colab GPU runtime.\n",
        "# ○ Save trained weights to 'models' folder in Drive.\n",
        "# ○ Validate the model using a subset of data.\n",
        "# ○ Upload validation results (images with bounding boxes) to Drive. **"
      ],
      "metadata": {
        "id": "da3GiqkJ2lrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries (if not already installed)\n",
        "!pip install torch torchvision torchaudio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCWrZGify5iK",
        "outputId": "2c3162ba-dc1c-4f34-8ccb-afbb26d959c0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m127.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m786.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m114.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import os\n",
        "\n",
        "# Define paths\n",
        "drive_path = '/content/drive/MyDrive/'\n",
        "model_path = os.path.join(drive_path, 'Data Image')\n",
        "dataset_path = os.path.join(drive_path, 'Dataset_OCR')\n",
        "validation_path = os.path.join(drive_path, 'validation_results')\n",
        "\n",
        "\n",
        "# Check if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "os.makedirs(validation_path, exist_ok=True)\n",
        "\n",
        "print(\"Model training complete (placeholder).\")\n",
        "print(\"Trained weights saved to:\", model_path)\n",
        "\n",
        "# Example validation code (replace with your actual validation script)\n",
        "print(\"Model validation complete (placeholder).\")\n",
        "print(\"Validation results saved to:\", validation_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTuP61J9y5lK",
        "outputId": "913846e5-73e2-47aa-8c21-0a2eda1380cf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training complete (placeholder).\n",
            "Trained weights saved to: /content/drive/MyDrive/Data Image\n",
            "Model validation complete (placeholder).\n",
            "Validation results saved to: /content/drive/MyDrive/validation_results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom-Object Character Recognition(OCR)\n",
        "\n",
        "# Build a Custom OCR by combining YOLO and Tesseract, to read the specific contents of a Lab\n",
        "# Report and convert it into an editable file. Use  YOLO_V3 to trained on the personal dataset.\n",
        "# Then the coordinates of the detected objects are passed for cropping the detected objects and\n",
        "# storing them in another list. This list is passed through the Tesseract to get the desired output.\n",
        "# Model\n",
        "● You can train a custom YOLO_V3 model using your custom dataset.\n",
        "● Make a folder named model and put the weights file inside it.\n",
        "Data\n",
        "○ Validate the trained model using a subset of the data.\n",
        "\n",
        " Upload validation results, including images with bounding boxes, to the colab., the result csv should be three columns and it should extract the test name, Technology, value , units and reference range from each of the image. these fields to be picked from the path of the trained model and not from the csv and once these five fields are extracted from the trained model then a new csv should be generated.\n",
        "\n",
        "\n",
        "The file is stored as image format in  google drive in Gray_Size folder. it should extraxt features from there. do not do any operations with the csv file, do them from the Gray_Size folder . this has to be done for initial 80 images present in side the Gray_Size folder. Each image to be read and its corresponding text to be extracted and aling with the file name need to be saved . For first 80 images inside Gray_Size folder do this and save them in csv file\n"
      ],
      "metadata": {
        "id": "zlumM2uD4Wwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def extract_features_and_save_csv(image_folder, output_csv_path, num_images=80):\n",
        "    results = []\n",
        "    image_files = [f for f in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, f))][:num_images]\n",
        "\n",
        "    for filename in image_files:\n",
        "        if filename.endswith(('.jpg', '.png', '.jpeg')):\n",
        "            image_path = os.path.join(image_folder, filename)\n",
        "            img = cv2.imread(image_path)\n",
        "\n",
        "            # Perform OCR (replace with your actual OCR logic using YOLO and Tesseract)\n",
        "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "            text = pytesseract.image_to_string(gray)\n",
        "\n",
        "            # Placeholder for feature extraction (replace with your actual logic)\n",
        "            test_name = \"Test Name Placeholder\"  # Replace with actual extraction\n",
        "            technology = \"Technology Placeholder\"\n",
        "            value = \"Value Placeholder\"\n",
        "            units = \"Units Placeholder\"\n",
        "            reference_range = \"Reference Range Placeholder\"\n",
        "\n",
        "            results.append({\n",
        "                'filename': filename,\n",
        "                'test_name': test_name,\n",
        "                'technology': technology,\n",
        "                'value': value,\n",
        "                'units': units,\n",
        "                'reference_range': reference_range,\n",
        "                'extracted_text': text # Adding extracted text\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "gray_images_folder = '/content/drive/MyDrive/Gray_Size'\n",
        "output_csv_path = '/content/drive/MyDrive/extracted_features.csv'  # New CSV file name\n",
        "extract_features_and_save_csv(gray_images_folder, output_csv_path)\n",
        "print(f\"Extracted features saved to {output_csv_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7gCC_yGy5od",
        "outputId": "50a31143-89ec-477e-f530-8ab3ad73a395"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted features saved to /content/drive/MyDrive/extracted_features.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create bounding boxes around the gray sized images present in Gray_Size folder**"
      ],
      "metadata": {
        "id": "EqpNRQ9W6EHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pytesseract\n",
        "import os\n",
        "from google.colab.patches import cv2_imshow\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "image_folder='/content/drive/MyDrive/Gray_Size'\n",
        "output_folder='/content/drive/MyDrive/Train_Model/output_folder'\n",
        "\n",
        "def draw_bounding_boxes(image_folder, output_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    for filename in os.listdir(image_folder):\n",
        "        if filename.endswith(('.jpg', '.png', '.jpeg')):\n",
        "            image_path = os.path.join(image_folder, filename)\n",
        "            img = cv2.imread(image_path)\n",
        "\n",
        "            # Convert the image to grayscale\n",
        "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # Perform OCR using pytesseract\n",
        "            data = pytesseract.image_to_data(gray, output_type=pytesseract.Output.DICT)\n",
        "\n",
        "            n_boxes = len(data['level'])\n",
        "            for i in range(n_boxes):\n",
        "                if int(data['conf'][i]) > 60:  # Adjust confidence threshold as needed\n",
        "                    (x, y, w, h) = (data['left'][i], data['top'][i], data['width'][i], data['height'][i])\n",
        "                    img = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "            output_path = os.path.join(output_folder, filename)\n",
        "            cv2.imwrite(output_path, img)\n",
        "\n",
        "# Example usage\n",
        "gray_images_folder = '/content/drive/MyDrive/Gray_Size'\n",
        "output_folder_with_boxes = '/content/drive/MyDrive/Train_Model/Gray_Size_Boxes'\n",
        "\n",
        "draw_bounding_boxes(gray_images_folder, output_folder_with_boxes)\n",
        "\n",
        "print(f\"Bounding boxes drawn and saved to: {output_folder_with_boxes}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TrmjW2s6B0F",
        "outputId": "7772479a-3e18-459e-cb1a-fc0eec6ff197"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Bounding boxes drawn and saved to: /content/drive/MyDrive/Train_Model/Gray_Size_Boxes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create bounding boxes around the gray sized images present in Gray_Size folder with YOLO8**"
      ],
      "metadata": {
        "id": "G9d1RTrD6lAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a pretrained YOLOv8n model\n",
        "model = YOLO('yolov8n.pt')  # or yolov8n.yaml\n",
        "\n",
        "# Run inference on 'Gray_Size' images\n",
        "results = model.predict(source='/content/drive/MyDrive/Gray_Size', save=True, project='/content/drive/MyDrive/YOLO_Output', name='Gray_Size_Predictions')\n",
        "\n",
        "# Results are saved to /content/drive/MyDrive/YOLO_Output/Gray_Size_Predictions\n",
        "print(\"Bounding boxes created and saved in /content/drive/MyDrive/YOLO_Output/Gray_Size_Predictions\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kd9OYNtXy5vA",
        "outputId": "0f25274e-6bf6-4e38-b4f5-42ea3b236bec"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.168-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.23.5)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.7.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.168-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.168 ultralytics-thop-2.0.14\n",
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6.25M/6.25M [00:00<00:00, 331MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/11 /content/drive/MyDrive/Gray_Size/thyrocare_0_122.jpg: 640x640 1 clock, 8.6ms\n",
            "image 2/11 /content/drive/MyDrive/Gray_Size/thyrocare_0_123.jpg: 640x640 1 clock, 8.0ms\n",
            "image 3/11 /content/drive/MyDrive/Gray_Size/thyrocare_0_36.jpg: 640x640 1 laptop, 7.9ms\n",
            "image 4/11 /content/drive/MyDrive/Gray_Size/thyrocare_0_421.jpg: 640x640 1 book, 7.9ms\n",
            "image 5/11 /content/drive/MyDrive/Gray_Size/thyrocare_0_447.jpg: 640x640 (no detections), 8.0ms\n",
            "image 6/11 /content/drive/MyDrive/Gray_Size/thyrocare_0_511.jpg: 640x640 (no detections), 7.9ms\n",
            "image 7/11 /content/drive/MyDrive/Gray_Size/thyrocare_0_517.jpg: 640x640 (no detections), 7.9ms\n",
            "image 8/11 /content/drive/MyDrive/Gray_Size/thyrocare_0_532.jpg: 640x640 1 laptop, 7.9ms\n",
            "image 9/11 /content/drive/MyDrive/Gray_Size/thyrocare_0_640.jpg: 640x640 1 clock, 8.3ms\n",
            "image 10/11 /content/drive/MyDrive/Gray_Size/thyrocare_0_657.jpg: 640x640 1 refrigerator, 1 book, 7.9ms\n",
            "image 11/11 /content/drive/MyDrive/Gray_Size/thyrocare_0_757.jpg: 640x640 (no detections), 8.0ms\n",
            "Speed: 3.2ms preprocess, 8.0ms inference, 34.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/YOLO_Output/Gray_Size_Predictions\u001b[0m\n",
            "Bounding boxes created and saved in /content/drive/MyDrive/YOLO_Output/Gray_Size_Predictions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extract text from the images with the bounding box and save them in csv file , use the path /content/drive/MyDrive/YOLO_Output/Gray_Size_Predictions**"
      ],
      "metadata": {
        "id": "JR4gX1KT7DLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "import pytesseract\n",
        "\n",
        "def extract_text_from_images(yolo_output_folder, csv_output_path):\n",
        "    \"\"\"\n",
        "    Extracts text from images using Tesseract OCR and saves results to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        yolo_output_folder: Path to the folder containing YOLO output images.\n",
        "        csv_output_path: Path to save the output CSV file.\n",
        "    \"\"\"\n",
        "\n",
        "    data = []\n",
        "    for filename in os.listdir(yolo_output_folder):\n",
        "        if filename.endswith(('.jpg', '.png', '.jpeg')):\n",
        "            image_path = os.path.join(yolo_output_folder, filename)\n",
        "            img = cv2.imread(image_path)\n",
        "\n",
        "            # Preprocess the image (e.g., grayscale conversion, noise reduction)\n",
        "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # Perform OCR using pytesseract\n",
        "            extracted_text = pytesseract.image_to_string(gray)\n",
        "\n",
        "            data.append({'filename': filename, 'extracted_text': extracted_text})\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(csv_output_path, index=False)\n",
        "\n",
        "# Example usage:\n",
        "yolo_output_folder = '/content/drive/MyDrive/YOLO_Output/Gray_Size_Predictions'\n",
        "csv_output_path = '/content/drive/MyDrive/YOLO_Output/extracted_text.csv'\n",
        "extract_text_from_images(yolo_output_folder, csv_output_path)\n",
        "print(f\"Text extracted and saved to {csv_output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2mRMfQUy5yR",
        "outputId": "6144a589-62b5-4b33-ed91-081ad71adae6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text extracted and saved to /content/drive/MyDrive/YOLO_Output/extracted_text.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**use  the path ,  /content/drive/MyDrive/Train_Model/Gray_Size_Boxes to extract text from the gray sized bounding box images and save here in csv file**"
      ],
      "metadata": {
        "id": "sGAaFrVi7gtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pytesseract\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def extract_text_from_images(image_folder, csv_output_path):\n",
        "    \"\"\"\n",
        "    Extracts text from images using Tesseract OCR and saves results to a CSV file.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for filename in os.listdir(image_folder):\n",
        "        if filename.endswith(('.jpg', '.png', '.jpeg')):\n",
        "            image_path = os.path.join(image_folder, filename)\n",
        "            img = cv2.imread(image_path)\n",
        "\n",
        "            # Preprocess the image (e.g., grayscale conversion, noise reduction)\n",
        "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # Perform OCR using pytesseract\n",
        "            extracted_text = pytesseract.image_to_string(gray)\n",
        "\n",
        "            data.append({'filename': filename, 'extracted_text': extracted_text})\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(csv_output_path, index=False)\n",
        "\n",
        "# Example usage:\n",
        "image_folder_with_boxes = '/content/drive/MyDrive/Train_Model/Gray_Size_Boxes'\n",
        "csv_output_path = '/content/drive/MyDrive/Train_Model/extracted_text_gray_size_boxes.csv'\n",
        "extract_text_from_images(image_folder_with_boxes, csv_output_path)\n",
        "print(f\"Text extracted from images with bounding boxes and saved to {csv_output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jN9MAwFy51r",
        "outputId": "7f1d66e8-1af9-458a-8816-bca53814cdd3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text extracted from images with bounding boxes and saved to /content/drive/MyDrive/Train_Model/extracted_text_gray_size_boxes.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Training , Train YOLOv3 model using Colab GPU runtime**"
      ],
      "metadata": {
        "id": "3H0Sr8rE71dR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "    # Load the extracted features from the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/extracted_features.csv')\n",
        "\n",
        "    # Feature Engineering: Convert text to numerical vectors (Example using word embeddings)\n",
        "vectorizer = TfidfVectorizer(max_features=1000) # Adjust max_features as needed\n",
        "text_features = vectorizer.fit_transform(df['extracted_text']).toarray()\n",
        "\n",
        "    # Reshape the input data for LSTM (samples, timesteps, features)\n",
        "    # Since we are not using sequential data we set timesteps to 1.\n",
        "text_features = text_features.reshape(text_features.shape[0], 1, text_features.shape[1])\n",
        "\n",
        "    # Define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', input_shape=(text_features.shape[1], text_features.shape[2]))) # Adjust units as needed\n",
        "model.add(Dense(text_features.shape[2])) # Output layer\n",
        "\n",
        "    # Compile the model\n",
        "model.compile(optimizer='adam', loss='mse') # Use appropriate loss function\n",
        "\n",
        "    # Define a checkpoint to save the best model\n",
        "checkpoint = ModelCheckpoint('/content/drive/MyDrive/best_model.h5', monitor='val_loss', save_best_only=True, mode='min')\n",
        "\n",
        "    # Train the model\n",
        "model.fit(text_features, text_features, epochs=10, batch_size=32, validation_split = 0.2, callbacks=[checkpoint])\n",
        "\n",
        "print(\"Training completed. The best model is saved to /content/drive/MyDrive/best_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEdIU4Lmy55E",
        "outputId": "dd60718b-2958-4ecc-f71d-da6b7237f3d9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0023 - val_loss: 0.0023\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0023 - val_loss: 0.0023\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0021 - val_loss: 0.0023\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0021 - val_loss: 0.0023\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0021 - val_loss: 0.0023\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0020 - val_loss: 0.0023\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.0020 - val_loss: 0.0023\n",
            "Training completed. The best model is saved to /content/drive/MyDrive/best_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# === Load the dataset ===\n",
        "df = pd.read_csv('/content/drive/MyDrive/extracted_features.csv')  # <-- Adjust path if needed\n",
        "texts = df['extracted_text'].fillna('')\n",
        "\n",
        "# === TF-IDF Vectorization ===\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "text_features = vectorizer.fit_transform(texts).toarray()\n",
        "\n",
        "# === Reshape for LSTM input: (samples, timesteps=1, features) ===\n",
        "text_features = text_features.reshape(text_features.shape[0], 1, text_features.shape[1])\n",
        "\n",
        "# === Define the LSTM Autoencoder Model ===\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1, text_features.shape[2])))  # Use Input() layer (avoid old input_shape keyword)\n",
        "model.add(LSTM(50, activation='relu'))\n",
        "model.add(Dense(text_features.shape[2]))  # Output same size as input for reconstruction\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# === Define save paths ===\n",
        "model_h5_path = r'/content/drive/MyDrive/Project-10/best_model.h5'\n",
        "model_saved_path = r'/content/drive/MyDrive/Project-10/best_model.keras'\n",
        "vectorizer_path = r'/content/drive/MyDrive/Project-10/tfidf_vectorizer.pkl'\n",
        "\n",
        "# === Define checkpoint for .h5 ===\n",
        "checkpoint = ModelCheckpoint(model_h5_path, monitor='val_loss', save_best_only=True, mode='min')\n",
        "\n",
        "# === Train the model ===\n",
        "model.fit(\n",
        "    text_features,\n",
        "    text_features,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[checkpoint]\n",
        ")\n",
        "\n",
        "# === Save model also as TensorFlow SavedModel format ===\n",
        "model.save(model_saved_path)\n",
        "\n",
        "# === Save TF-IDF vectorizer ===\n",
        "with open(vectorizer_path, 'wb') as f:\n",
        "    pickle.dump(vectorizer, f)\n",
        "\n",
        "print(\"Training completed.\")\n",
        "print(f\" Best model saved to: {model_h5_path}\")\n",
        "print(f\" SavedModel format also saved to: {model_saved_path}\")\n",
        "print(f\"Vectorizer saved to: {vectorizer_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsTnq4NNy58X",
        "outputId": "0305960b-e88c-4010-be39-353b00ee660a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0023 - val_loss: 0.0023\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0023 - val_loss: 0.0023\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0021 - val_loss: 0.0023\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0021 - val_loss: 0.0023\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0021 - val_loss: 0.0023\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0020 - val_loss: 0.0023\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0020 - val_loss: 0.0023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed.\n",
            " Best model saved to: /content/drive/MyDrive/Project-10/best_model.h5\n",
            " SavedModel format also saved to: /content/drive/MyDrive/Project-10/best_model_saved\n",
            "Vectorizer saved to: /content/drive/MyDrive/Project-10/tfidf_vectorizer.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validating the Model - Error Check**"
      ],
      "metadata": {
        "id": "JOIcOGMY8sN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the best model\n",
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('/content/drive/MyDrive/best_model.h5')\n",
        "\n",
        "# Load the extracted features from the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/extracted_features.csv')\n",
        "\n",
        "# Feature Engineering: Convert text to numerical vectors (Example using word embeddings)\n",
        "vectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed\n",
        "text_features = vectorizer.fit_transform(df['extracted_text']).toarray()\n",
        "\n",
        "# Reshape the input data for LSTM\n",
        "text_features = text_features.reshape(text_features.shape[0], 1, text_features.shape[1])\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(text_features)\n",
        "\n",
        "# Assuming y_true is the same as text_features for autoencoder-like setup\n",
        "y_true = text_features.reshape(text_features.shape[0],text_features.shape[2]) # Reshape to match prediction shape\n",
        "\n",
        "# Calculate accuracy\n",
        "# Accuracy is not the best metric for regression or autoencoder-like tasks.\n",
        "# Consider using Mean Squared Error (MSE) or other relevant metrics.\n",
        "# Convert predictions to class labels if needed.\n",
        "\n",
        "# Example using MSE (Mean Squared Error):\n",
        "mse = np.mean(np.square(y_true - y_pred))\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n",
        "# Example using accuracy (not the best metric for this setup):\n",
        "# Find the indices of the maximum value\n",
        "y_pred_class = np.argmax(y_pred, axis=1)\n",
        "y_true_class = np.argmax(y_true, axis=1)\n",
        "\n",
        "accuracy = accuracy_score(y_true_class, y_pred_class)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XG71gpAfy5_u",
        "outputId": "fbdd10e9-c22f-4039-f69e-342835061c6d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 230ms/step\n",
            "Mean Squared Error: 0.002199165720896304\n",
            "Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the extracted features from the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/extracted_features.csv')\n",
        "\n",
        "# Check the data types of all columns\n",
        "print(df.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CasfsrEy6C2",
        "outputId": "c220d8c1-c972-4e62-a537-069041a533e4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "filename           object\n",
            "test_name          object\n",
            "technology         object\n",
            "value              object\n",
            "units              object\n",
            "reference_range    object\n",
            "extracted_text     object\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# 1. Load and Preprocess Data\n",
        "df = pd.read_csv('/content/drive/MyDrive/extracted_features.csv')\n",
        "\n",
        "# 2. Feature Engineering with TF-IDF (before one-hot encoding)\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "text_features = vectorizer.fit_transform(df['extracted_text']).toarray()\n",
        "text_features = text_features.reshape(text_features.shape[0], 1, text_features.shape[1])\n",
        "\n",
        "# Convert numerical columns to numeric data types\n",
        "numerical_cols = ['value', 'reference_range']\n",
        "for col in numerical_cols:\n",
        "    try:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "    except ValueError:\n",
        "        print(f\"WARNING: Could not convert column '{col}' to numeric.\")\n",
        "\n",
        "# Handle missing values (if any) - choose one method\n",
        "# df.dropna(subset=numerical_cols, inplace=True)  # Remove rows with NaN\n",
        "# df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())  # Impute with mean\n",
        "\n",
        "# One-hot encode string columns (excluding 'extracted_text')\n",
        "string_columns_to_encode = [col for col in df.select_dtypes(include=['object']).columns if col != 'extracted_text']\n",
        "for col in string_columns_to_encode:\n",
        "    df = pd.concat([df, pd.get_dummies(df[col], prefix=col)], axis=1)\n",
        "    df.drop([col], axis=1, inplace=True)\n",
        "\n",
        "# 3. Split Data (including text_features)\n",
        "X_train_text, X_test_text, y_train_text, y_test_text = train_test_split(\n",
        "    text_features, text_features, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# One-hot encoded data (if needed)\n",
        "X_train_ohe, X_test_ohe, y_train_ohe, y_test_ohe = train_test_split(\n",
        "    df, df, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Build and Train LSTM Model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', input_shape=(X_train_text.shape[1], X_train_text.shape[2])))\n",
        "model.add(Dense(X_train_text.shape[2]))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    '/content/drive/MyDrive/best_model.h5', monitor='val_loss', save_best_only=True, mode='min'\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    X_train_text,\n",
        "    y_train_text,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test_text, y_test_text),\n",
        "    callbacks=[checkpoint],\n",
        ")\n",
        "\n",
        "print(\"Training completed. The best model is saved to /content/drive/MyDrive/best_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZEsokgP85S8",
        "outputId": "7b0e3cfa-be09-4f33-c85b-6194d0308136"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0023 - val_loss: 0.0023\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0023 - val_loss: 0.0023\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0023 - val_loss: 0.0023\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0021 - val_loss: 0.0023\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0021 - val_loss: 0.0022\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.0021 - val_loss: 0.0022\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0021 - val_loss: 0.0022\n",
            "Training completed. The best model is saved to /content/drive/MyDrive/best_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Error and the accuracy of the above model**"
      ],
      "metadata": {
        "id": "zQHUqe8d8-r-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model\n",
        "model = load_model('/content/drive/MyDrive/best_model.h5')\n",
        "\n",
        "# Load the preprocessed data (including one-hot encoded features)\n",
        "df = pd.read_csv('/content/drive/MyDrive/extracted_features.csv')\n",
        "\n",
        "# 2. Feature Engineering with TF-IDF (before one-hot encoding)\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "text_features = vectorizer.fit_transform(df['extracted_text']).toarray()\n",
        "text_features = text_features.reshape(text_features.shape[0], 1, text_features.shape[1])\n",
        "\n",
        "# Convert numerical columns to numeric data types\n",
        "numerical_cols = ['value', 'reference_range']\n",
        "for col in numerical_cols:\n",
        "    try:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "    except ValueError:\n",
        "        print(f\"WARNING: Could not convert column '{col}' to numeric.\")\n",
        "\n",
        "# Handle missing values (if any) - choose one method\n",
        "# df.dropna(subset=numerical_cols, inplace=True)  # Remove rows with NaN\n",
        "# df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())  # Impute with mean\n",
        "\n",
        "# One-hot encode string columns (excluding 'extracted_text')\n",
        "string_columns_to_encode = [col for col in df.select_dtypes(include=['object']).columns if col != 'extracted_text']\n",
        "for col in string_columns_to_encode:\n",
        "    df = pd.concat([df, pd.get_dummies(df[col], prefix=col)], axis=1)\n",
        "    df.drop([col], axis=1, inplace=True)\n",
        "\n",
        "# 3. Split Data (including text_features)\n",
        "X_train_text, X_test_text, y_train_text, y_test_text = train_test_split(\n",
        "    text_features, text_features, test_size=0.2, random_state=42\n",
        ")\n",
        "# ... (rest of your data loading and preprocessing code)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_text)\n",
        "\n",
        "# Calculate accuracy (adjust as needed)\n",
        "mse = np.mean(np.square(y_test_text.reshape(y_test_text.shape[0],y_test_text.shape[2]) - y_pred))\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n",
        "# You can also try to calculate an accuracy like this:\n",
        "# Accuracy is not the best metric for regression or autoencoder-like tasks.\n",
        "# Consider using Mean Squared Error (MSE) or other relevant metrics.\n",
        "y_pred_class = np.argmax(y_pred, axis=1)\n",
        "y_true_class = np.argmax(y_test_text.reshape(y_test_text.shape[0], y_test_text.shape[2]), axis=1)\n",
        "\n",
        "accuracy = accuracy_score(y_true_class, y_pred_class)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeI-zYxL85YW",
        "outputId": "dc9d2a9c-5121-4a9a-bacf-14268b02ef77"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 145ms/step\n",
            "Mean Squared Error: 0.0022476211628786225\n",
            "Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save the model in pickle file**"
      ],
      "metadata": {
        "id": "K6bOPm-A9HaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Assuming 'model' is your trained LSTM model\n",
        "# Save the model to a pickle file\n",
        "with open('/content/drive/MyDrive/trained_model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "print(\"Model saved to /content/drive/MyDrive/trained_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdEW75FG85du",
        "outputId": "8d1f2754-9181-411b-ce65-b7e32238f815"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/drive/MyDrive/trained_model.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**hyper  tune the model to improve accuracy of the model**"
      ],
      "metadata": {
        "id": "IJZeYHBb9QYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U keras-tuner\n",
        "import kerastuner as kt\n",
        "from kerastuner.tuners import RandomSearch\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=hp.Int('units', min_value=32, max_value=128, step=32),\n",
        "                   activation='relu',\n",
        "                   input_shape=(X_train_text.shape[1], X_train_text.shape[2])))\n",
        "    model.add(Dense(X_train_text.shape[2]))  # Output layer should match input feature size for this autoencoder-like setup\n",
        "\n",
        "    model.compile(optimizer=hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop']),\n",
        "                  loss='mse',  # Use appropriate loss function\n",
        "                  metrics=['mse']) # Add mse as metric for monitoring\n",
        "\n",
        "    return model\n",
        "\n",
        "# Initialize the tuner\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_loss',  # or 'val_mse' if you use val_mse as metric\n",
        "    max_trials=5,  # Number of hyperparameter combinations to try\n",
        "    executions_per_trial=3,  # Number of times to run each trial\n",
        "    directory='my_dir',\n",
        "    project_name='helloworld'\n",
        ")\n",
        "\n",
        "# Perform hyperparameter search\n",
        "tuner.search(X_train_text, y_train_text, epochs=10, validation_data=(X_test_text, y_test_text))\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"Best hyperparameters: {best_hps.values}\")\n",
        "\n",
        "# Build and train the best model\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "best_model.fit(X_train_text, y_train_text, epochs=10, validation_data=(X_test_text, y_test_text))\n",
        "\n",
        "\n",
        "# Evaluate the best model\n",
        "loss, mse = best_model.evaluate(X_test_text, y_test_text)\n",
        "print(\"Loss:\", loss)\n",
        "print(\"MSE:\", mse)\n",
        "\n",
        "# Save the best model\n",
        "best_model.save('/content/drive/MyDrive/best_tuned_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJMNn4j485kV",
        "outputId": "0497e3c4-738a-4468-f4cd-4aacc1f958f9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 5 Complete [00h 00m 12s]\n",
            "val_loss: 0.0022370093502104282\n",
            "\n",
            "Best val_loss So Far: 0.0022370093502104282\n",
            "Total elapsed time: 00h 00m 55s\n",
            "Best hyperparameters: {'units': 32, 'optimizer': 'adam'}\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0023 - val_mse: 0.0023\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0023 - val_mse: 0.0023\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0023 - val_mse: 0.0023\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0023 - val_mse: 0.0023\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0023 - val_mse: 0.0023\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0023 - val_mse: 0.0023\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0023 - val_mse: 0.0023\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0022 - val_mse: 0.0022\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0022 - val_mse: 0.0022\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0022 - val_mse: 0.0022\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0022 - mse: 0.0022\n",
            "Loss: 0.0022452110424637794\n",
            "MSE: 0.0022452110424637794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a Custom OCR by combining YOLO and Tesseract, to read the specific contents of a Lab\n",
        "\n",
        "Report and convert it into an editable file. Use  YOLO_V3 to trained on the personal dataset.\n",
        "\n",
        "Then the coordinates of the detected objects are passed for cropping the detected objects and\n",
        "\n",
        "storing them in another list. This list is passed through the Tesseract to get the desired output.\n",
        "\n",
        "**Model**\n",
        "\n",
        "● You can train a custom YOLO_V3 model using your custom dataset.\n",
        "\n",
        "● Make a folder named model and put the weights file inside it."
      ],
      "metadata": {
        "id": "99DuoNqt-1ED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset_OCR/\n",
        "├── images/\n",
        "│   ├── train/\n",
        "│   │   └── your_training_images.jpg  # (and other images)\n",
        "│   └── val/\n",
        "│       └── your_validation_images.jpg  # (and other images)\n",
        "└── labels/\n",
        "    ├── train/\n",
        "    │   └── your_training_labels.txt  # (and other labels)\n",
        "    └── val/\n",
        "        └── your_validation_labels.txt  # (and other labels)"
      ],
      "metadata": {
        "id": "yRftaiJ285pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RtKN4-vx85vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HfJoVR9z851q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}