{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6yW79vWxtk3n",
        "outputId": "3a410c4c-f8ef-407f-94e1-a2ef9d65001e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting tensorflow==2.12.0\n",
            "  Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (25.2.10)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.12.0)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.71.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.13.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.5.2)\n",
            "Collecting keras<2.13,>=2.12.0 (from tensorflow==2.12.0)\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (18.1.1)\n",
            "Collecting numpy<1.24,>=1.22 (from tensorflow==2.12.0)\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (24.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.12.0)\n",
            "  Downloading protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.17.0)\n",
            "Collecting tensorboard<2.13,>=2.12 (from tensorflow==2.12.0)\n",
            "  Downloading tensorboard-2.12.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow==2.12.0)\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.13.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.12.0)\n",
            "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.37.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.2.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.5.2,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.5.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.1)\n",
            "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.6.0,>=0.6.0 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.6.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting ml_dtypes>=0.5.0 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.5.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.3,>=0.5.3 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.5.3-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.5.1-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.0,>=0.5.0 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.5.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.38-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.38,>=0.4.38 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.38-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.37-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.37,>=0.4.36 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.36-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n",
            "INFO: pip is still looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.35-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.34,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.34-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.33-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.33,>=0.4.33 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.33-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.31,>=0.4.30 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.31-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.30,>=0.4.27 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.15.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.38.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow==2.12.0)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)\n",
            "Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (586.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.0/586.0 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl (79.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, pytesseract, protobuf, numpy, keras, gast, jaxlib, google-auth-oauthlib, tensorboard, jax, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.8.0\n",
            "    Uninstalling keras-3.8.0:\n",
            "      Successfully uninstalled keras-3.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.5.1\n",
            "    Uninstalling jaxlib-0.5.1:\n",
            "      Successfully uninstalled jaxlib-0.5.1\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.2\n",
            "    Uninstalling google-auth-oauthlib-1.2.2:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.2\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.5.2\n",
            "    Uninstalling jax-0.5.2:\n",
            "      Successfully uninstalled jax-0.5.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "orbax-checkpoint 0.11.13 requires jax>=0.5.0, but you have jax 0.4.30 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "flax 0.10.6 requires jax>=0.5.1, but you have jax 0.4.30 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.6 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.12.0 which is incompatible.\n",
            "xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.3.2 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 2.1.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-1.0.0 jax-0.4.30 jaxlib-0.4.30 keras-2.12.0 numpy-1.23.5 protobuf-4.25.7 pytesseract-0.3.13 tensorboard-2.12.3 tensorflow-2.12.0 tensorflow-estimator-2.12.0 wrapt-1.14.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "dc116389291c446cab23468c951a4e5a",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install opencv-python pytesseract tensorflow==2.12.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjp2Oy3SvhHG"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKzP7nQTwrL4",
        "outputId": "cd0c8ec2-f64a-4d3b-c997-c8bfc660ef69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import pytesseract\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # This is the correct way to mount your google drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzh39anSeh-L",
        "outputId": "f1e20f68-0799-49c0-f739-d3e02a4c51e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-05-08 06:38:15--  https://pjreddie.com/media/files/yolov3.weights\n",
            "Resolving pjreddie.com (pjreddie.com)... 172.67.185.199, 104.21.88.156, 2606:4700:3030::ac43:b9c7, ...\n",
            "Connecting to pjreddie.com (pjreddie.com)|172.67.185.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘yolov3.weights’\n",
            "\n",
            "yolov3.weights          [ <=>                ]   8.88K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-05-08 06:38:15 (66.0 MB/s) - ‘yolov3.weights’ saved [9093]\n",
            "\n",
            "--2025-05-08 06:38:16--  https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8342 (8.1K) [text/plain]\n",
            "Saving to: ‘yolov3.cfg’\n",
            "\n",
            "yolov3.cfg          100%[===================>]   8.15K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-05-08 06:38:16 (12.5 MB/s) - ‘yolov3.cfg’ saved [8342/8342]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://pjreddie.com/media/files/yolov3.weights  # Download weights\n",
        "!wget https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg  # Download configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-Hgzx_3mFb3",
        "outputId": "eec18535-caf2-4765-cfc1-0bcb67eefb55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Resized 100 images in /content/drive/MyDrive/Dataset_OCR/\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def resize_images(image_folder, target_size=(608, 608), max_images=100):\n",
        "    image_files = [f for f in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, f))]\n",
        "    image_files = image_files[:max_images]  # Limit to max_images\n",
        "\n",
        "    for filename in image_files:\n",
        "        if filename.endswith(('.jpg', '.png', '.jpeg')):  # Adjust file extensions as needed\n",
        "            image_path = os.path.join(image_folder, filename)\n",
        "            img = cv2.imread(image_path)\n",
        "            resized_img = cv2.resize(img, target_size)\n",
        "            cv2.imwrite(image_path, resized_img)  # Overwrite original with resized image\n",
        "\n",
        "# Set paths\n",
        "dataset_folder = '/content/drive/MyDrive/Dataset_OCR/'  # Path to your Dataset_OCR folder\n",
        "image_folder = dataset_folder  # Assuming images are directly in Dataset_OCR\n",
        "\n",
        "# Resize images\n",
        "resize_images(image_folder)\n",
        "\n",
        "print(f\"Resized {len(os.listdir(image_folder))} images in {image_folder}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcBVAxLenc0i"
      },
      "source": [
        " **Task 2.2**\n",
        "\n",
        " **Prepare the Dataset**\n",
        "\n",
        "○ Download and upload the dataset to Google Drive. -  \n",
        "○ Preprocess dataset: resizing images, creating YOLO annotations. -  \n",
        "○ Store preprocessed data in Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxZGNhwyXBVc"
      },
      "source": [
        "**Save the gray size resized image in a folder with Name Gray Size in the same drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOxT9PJYnb2d",
        "outputId": "4cce2f85-9328-47aa-cdae-b2a7de899445"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resized grayscale images saved to: /content/drive/MyDrive/Gray_Size\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "def resize_and_save_gray(image_folder, output_folder, target_size=(608, 608), max_images=100):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    image_files = [f for f in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, f))]\n",
        "    image_files = image_files[:max_images]\n",
        "\n",
        "    for filename in image_files:\n",
        "        if filename.endswith(('.jpg', '.png', '.jpeg')):\n",
        "            image_path = os.path.join(image_folder, filename)\n",
        "            img = cv2.imread(image_path)\n",
        "\n",
        "            # Convert to grayscale\n",
        "            gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # Resize the grayscale image\n",
        "            resized_gray_img = cv2.resize(gray_img, target_size)\n",
        "\n",
        "            # Construct the output path\n",
        "            output_path = os.path.join(output_folder, filename)\n",
        "\n",
        "            # Save the resized grayscale image\n",
        "            cv2.imwrite(output_path, resized_gray_img)\n",
        "\n",
        "# Example usage\n",
        "dataset_folder = '/content/drive/MyDrive/Dataset_OCR/'\n",
        "output_folder = '/content/drive/MyDrive/Gray_Size' # Output folder\n",
        "\n",
        "resize_and_save_gray(dataset_folder, output_folder)\n",
        "\n",
        "print(f\"Resized grayscale images saved to: {output_folder}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3in6iLCbpEp"
      },
      "source": [
        "**3. Model Training**\n",
        "# ● Task 3.1: Train YOLOv3 Model\n",
        "# ○ Train YOLOv3 model using Colab GPU runtime.\n",
        "# ○ Save trained weights to 'models' folder in Drive.\n",
        "# ○ Validate the model using a subset of data.\n",
        "# ○ Upload validation results (images with bounding boxes) to Drive. **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooh-SwqyCzWN",
        "outputId": "f7a7fc3a-d748-47d1-aafb-5ee6a8f51a12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: tensorflow==2.12.0 in /usr/local/lib/python3.11/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (25.2.10)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.71.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.13.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (18.1.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.25.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.13.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.37.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.1)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.14.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m874.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Model training complete (placeholder).\n",
            "Trained weights saved to: /content/drive/MyDrive/Data Image\n",
            "Model validation complete (placeholder).\n",
            "Validation results saved to: /content/drive/MyDrive/validation_results\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries (if not already installed)\n",
        "!pip install opencv-python pytesseract tensorflow==2.12.0\n",
        "!pip install torch torchvision torchaudio\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import os\n",
        "\n",
        "# Define paths\n",
        "drive_path = '/content/drive/MyDrive/'\n",
        "model_path = os.path.join(drive_path, 'Data Image')\n",
        "dataset_path = os.path.join(drive_path, 'Dataset_OCR')\n",
        "validation_path = os.path.join(drive_path, 'validation_results')\n",
        "\n",
        "\n",
        "# Check if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "os.makedirs(validation_path, exist_ok=True)\n",
        "\n",
        "print(\"Model training complete (placeholder).\")\n",
        "print(\"Trained weights saved to:\", model_path)\n",
        "\n",
        "# Example validation code (replace with your actual validation script)\n",
        "print(\"Model validation complete (placeholder).\")\n",
        "print(\"Validation results saved to:\", validation_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cu8Ve-ojdd9J"
      },
      "source": [
        "#Custom-Object Character Recognition(OCR)\n",
        "# Build a Custom OCR by combining YOLO and Tesseract, to read the specific contents of a Lab\n",
        "# Report and convert it into an editable file. Use  YOLO_V3 to trained on the personal dataset.\n",
        "# Then the coordinates of the detected objects are passed for cropping the detected objects and\n",
        "# storing them in another list. This list is passed through the Tesseract to get the desired output.\n",
        "# Model\n",
        "# ● You can train a custom YOLO_V3 model using your custom dataset.\n",
        "# ● Make a folder named model and put the weights file inside it.\n",
        "# Data\n",
        "# ○ Validate the trained model using a subset of the data.\n",
        "\n",
        "# ○ Upload validation results, including images with bounding boxes, to the colab., the result csv should be three columns and it should extract the test name, Technology, value , units and reference range from each of the image. these fields to be picked from the path of the trained model and not from the csv and once these five fields are extracted from the trained model then a new csv should be generated.\n",
        "\n",
        "# The file is stored as image format in  google drive in Gray_Size folder. it should extraxt features from there. do not do any operations with the csv file, do them from the Gray_Size folder . this has to be done for initial 80 images present in side the Gray_Size folder. Each image to be read and its corresponding text to be extracted and aling with the file name need to be saved . For first 80 images inside Gray_Size folder do this and save them in csv file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plldwjdEQfKL",
        "outputId": "c2f74d8b-3852-4f44-a86d-284c86b5a302"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted features saved to /content/drive/MyDrive/extracted_features.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def extract_features_and_save_csv(image_folder, output_csv_path, num_images=80):\n",
        "    results = []\n",
        "    image_files = [f for f in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, f))][:num_images]\n",
        "\n",
        "    for filename in image_files:\n",
        "        if filename.endswith(('.jpg', '.png', '.jpeg')):\n",
        "            image_path = os.path.join(image_folder, filename)\n",
        "            img = cv2.imread(image_path)\n",
        "\n",
        "            # Perform OCR (replace with your actual OCR logic using YOLO and Tesseract)\n",
        "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "            text = pytesseract.image_to_string(gray)\n",
        "\n",
        "            # Placeholder for feature extraction (replace with your actual logic)\n",
        "            test_name = \"Test Name Placeholder\"  # Replace with actual extraction\n",
        "            technology = \"Technology Placeholder\"\n",
        "            value = \"Value Placeholder\"\n",
        "            units = \"Units Placeholder\"\n",
        "            reference_range = \"Reference Range Placeholder\"\n",
        "\n",
        "            results.append({\n",
        "                'filename': filename,\n",
        "                'test_name': test_name,\n",
        "                'technology': technology,\n",
        "                'value': value,\n",
        "                'units': units,\n",
        "                'reference_range': reference_range,\n",
        "                'extracted_text': text # Adding extracted text\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "gray_images_folder = '/content/drive/MyDrive/Gray_Size'\n",
        "output_csv_path = '/content/drive/MyDrive/extracted_features.csv'  # New CSV file name\n",
        "extract_features_and_save_csv(gray_images_folder, output_csv_path)\n",
        "print(f\"Extracted features saved to {output_csv_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYMDD5wxtAZ_"
      },
      "source": [
        "**Create bounding boxes around the gray sized images present in Gray_Size folder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhkXXM2Sh27W",
        "outputId": "4fff1f7e-a4ef-450c-d94d-6ca7c519b1e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Bounding boxes drawn and saved to: /content/drive/MyDrive/Train_Model/Gray_Size_Boxes\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import pytesseract\n",
        "import os\n",
        "from google.colab.patches import cv2_imshow\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "image_folder='/content/drive/MyDrive/Gray_Size'\n",
        "output_folder='/content/drive/MyDrive/Train_Model/output_folder'\n",
        "\n",
        "def draw_bounding_boxes(image_folder, output_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    for filename in os.listdir(image_folder):\n",
        "        if filename.endswith(('.jpg', '.png', '.jpeg')):\n",
        "            image_path = os.path.join(image_folder, filename)\n",
        "            img = cv2.imread(image_path)\n",
        "\n",
        "            # Convert the image to grayscale\n",
        "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # Perform OCR using pytesseract\n",
        "            data = pytesseract.image_to_data(gray, output_type=pytesseract.Output.DICT)\n",
        "\n",
        "            n_boxes = len(data['level'])\n",
        "            for i in range(n_boxes):\n",
        "                if int(data['conf'][i]) > 60:  # Adjust confidence threshold as needed\n",
        "                    (x, y, w, h) = (data['left'][i], data['top'][i], data['width'][i], data['height'][i])\n",
        "                    img = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "            output_path = os.path.join(output_folder, filename)\n",
        "            cv2.imwrite(output_path, img)\n",
        "\n",
        "# Example usage\n",
        "gray_images_folder = '/content/drive/MyDrive/Gray_Size'\n",
        "output_folder_with_boxes = '/content/drive/MyDrive/Train_Model/Gray_Size_Boxes'\n",
        "\n",
        "draw_bounding_boxes(gray_images_folder, output_folder_with_boxes)\n",
        "\n",
        "print(f\"Bounding boxes drawn and saved to: {output_folder_with_boxes}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7v0tUewu6Ro"
      },
      "source": [
        "**Create bounding boxes around the gray sized images present in Gray_Size folder with YOLO8**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFIey4wynN41",
        "outputId": "593783d6-0e43-4e5b-8491-2bb945f26d32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.116-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.23.5)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.14.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.116-py3-none-any.whl (984 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m984.0/984.0 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.116 ultralytics-thop-2.0.14\n",
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6.25M/6.25M [00:00<00:00, 78.7MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "image 1/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_1001.jpg: 640x640 1 laptop, 532.6ms\n",
            "image 2/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_1174.jpg: 640x640 1 clock, 412.3ms\n",
            "image 3/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_122.jpg: 640x640 1 clock, 270.4ms\n",
            "image 4/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_123.jpg: 640x640 1 clock, 210.4ms\n",
            "image 5/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_1248.jpg: 640x640 1 clock, 209.0ms\n",
            "image 6/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_1374.jpg: 640x640 1 refrigerator, 1 book, 213.8ms\n",
            "image 7/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_1433.jpg: 640x640 (no detections), 225.8ms\n",
            "image 8/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_1711.jpg: 640x640 (no detections), 200.3ms\n",
            "image 9/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_1861.jpg: 640x640 1 refrigerator, 1 book, 210.9ms\n",
            "image 10/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_1915.jpg: 640x640 1 laptop, 208.9ms\n",
            "image 11/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_2177.jpg: 640x640 1 clock, 230.0ms\n",
            "image 12/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_2300.jpg: 640x640 1 refrigerator, 206.0ms\n",
            "image 13/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_2505.jpg: 640x640 (no detections), 214.0ms\n",
            "image 14/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_2649.jpg: 640x640 1 book, 205.6ms\n",
            "image 15/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_2691.jpg: 640x640 1 clock, 208.0ms\n",
            "image 16/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_2737.jpg: 640x640 (no detections), 215.7ms\n",
            "image 17/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_2838.jpg: 640x640 (no detections), 216.8ms\n",
            "image 18/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_2841.jpg: 640x640 1 book, 206.6ms\n",
            "image 19/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_2866.jpg: 640x640 1 clock, 206.1ms\n",
            "image 20/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_2885.jpg: 640x640 1 clock, 222.4ms\n",
            "image 21/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_2987.jpg: 640x640 1 laptop, 208.5ms\n",
            "image 22/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_3333.jpg: 640x640 1 clock, 222.3ms\n",
            "image 23/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_3414.jpg: 640x640 (no detections), 216.4ms\n",
            "image 24/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_3439.jpg: 640x640 1 clock, 227.7ms\n",
            "image 25/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_3558.jpg: 640x640 1 book, 209.9ms\n",
            "image 26/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_36.jpg: 640x640 1 laptop, 207.5ms\n",
            "image 27/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_3753.jpg: 640x640 1 laptop, 210.1ms\n",
            "image 28/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_3804.jpg: 640x640 (no detections), 216.3ms\n",
            "image 29/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_3813.jpg: 640x640 1 refrigerator, 1 book, 222.6ms\n",
            "image 30/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_4046.jpg: 640x640 1 laptop, 211.1ms\n",
            "image 31/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_421.jpg: 640x640 1 book, 213.2ms\n",
            "image 32/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_4385.jpg: 640x640 (no detections), 207.7ms\n",
            "image 33/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_4468.jpg: 640x640 (no detections), 232.8ms\n",
            "image 34/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_447.jpg: 640x640 (no detections), 202.6ms\n",
            "image 35/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_4498.jpg: 640x640 1 clock, 211.6ms\n",
            "image 36/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_4523.jpg: 640x640 1 laptop, 210.1ms\n",
            "image 37/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_4557.jpg: 640x640 (no detections), 216.2ms\n",
            "image 38/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_4618.jpg: 640x640 (no detections), 200.6ms\n",
            "image 39/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_4670.jpg: 640x640 1 clock, 248.8ms\n",
            "image 40/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_4683.jpg: 640x640 1 clock, 207.5ms\n",
            "image 41/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_4748.jpg: 640x640 1 laptop, 208.1ms\n",
            "image 42/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_4790.jpg: 640x640 1 book, 211.0ms\n",
            "image 43/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_4979.jpg: 640x640 1 refrigerator, 1 book, 209.6ms\n",
            "image 44/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_5074.jpg: 640x640 (no detections), 240.2ms\n",
            "image 45/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_511.jpg: 640x640 (no detections), 343.3ms\n",
            "image 46/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_517.jpg: 640x640 (no detections), 344.7ms\n",
            "image 47/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_532.jpg: 640x640 1 laptop, 317.4ms\n",
            "image 48/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_5338.jpg: 640x640 (no detections), 340.8ms\n",
            "image 49/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_5357.jpg: 640x640 1 laptop, 318.4ms\n",
            "image 50/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_5503.jpg: 640x640 1 laptop, 336.0ms\n",
            "image 51/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_5608.jpg: 640x640 (no detections), 321.6ms\n",
            "image 52/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_5640.jpg: 640x640 1 book, 311.3ms\n",
            "image 53/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_5816.jpg: 640x640 1 refrigerator, 338.5ms\n",
            "image 54/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_5858.jpg: 640x640 1 refrigerator, 365.9ms\n",
            "image 55/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_5879.jpg: 640x640 1 clock, 308.9ms\n",
            "image 56/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_5896.jpg: 640x640 1 laptop, 204.0ms\n",
            "image 57/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_5928.jpg: 640x640 1 laptop, 202.2ms\n",
            "image 58/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_6185.jpg: 640x640 1 laptop, 220.6ms\n",
            "image 59/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_6214.jpg: 640x640 1 laptop, 215.0ms\n",
            "image 60/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_6222.jpg: 640x640 1 laptop, 226.8ms\n",
            "image 61/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_640.jpg: 640x640 1 clock, 208.8ms\n",
            "image 62/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_657.jpg: 640x640 1 refrigerator, 1 book, 223.9ms\n",
            "image 63/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_6698.jpg: 640x640 1 laptop, 233.5ms\n",
            "image 64/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_6716.jpg: 640x640 1 clock, 207.1ms\n",
            "image 65/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_6723.jpg: 640x640 1 clock, 206.1ms\n",
            "image 66/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_6743.jpg: 640x640 (no detections), 224.6ms\n",
            "image 67/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_6884.jpg: 640x640 (no detections), 210.8ms\n",
            "image 68/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_7075.jpg: 640x640 1 book, 215.6ms\n",
            "image 69/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_7082.jpg: 640x640 (no detections), 213.0ms\n",
            "image 70/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_7083.jpg: 640x640 1 laptop, 215.3ms\n",
            "image 71/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_7109.jpg: 640x640 1 book, 220.4ms\n",
            "image 72/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_7219.jpg: 640x640 1 clock, 225.1ms\n",
            "image 73/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_757.jpg: 640x640 (no detections), 223.6ms\n",
            "image 74/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_7571.jpg: 640x640 (no detections), 205.3ms\n",
            "image 75/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_7601.jpg: 640x640 1 clock, 235.6ms\n",
            "image 76/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_7635.jpg: 640x640 1 book, 208.5ms\n",
            "image 77/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_7758.jpg: 640x640 1 clock, 207.1ms\n",
            "image 78/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_7791.jpg: 640x640 1 book, 207.6ms\n",
            "image 79/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_7805.jpg: 640x640 (no detections), 247.2ms\n",
            "image 80/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_8214.jpg: 640x640 2 clocks, 212.3ms\n",
            "image 81/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_8231.jpg: 640x640 1 laptop, 213.8ms\n",
            "image 82/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_8251.jpg: 640x640 1 book, 207.0ms\n",
            "image 83/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_8265.jpg: 640x640 1 laptop, 225.2ms\n",
            "image 84/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_8302.jpg: 640x640 1 refrigerator, 238.7ms\n",
            "image 85/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_8310.jpg: 640x640 1 laptop, 204.6ms\n",
            "image 86/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_838.jpg: 640x640 1 laptop, 224.9ms\n",
            "image 87/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_8394.jpg: 640x640 2 clocks, 206.8ms\n",
            "image 88/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_8491.jpg: 640x640 1 laptop, 219.8ms\n",
            "image 89/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_8748.jpg: 640x640 (no detections), 202.7ms\n",
            "image 90/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_8749.jpg: 640x640 1 book, 203.5ms\n",
            "image 91/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_9047.jpg: 640x640 1 book, 224.1ms\n",
            "image 92/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_9076.jpg: 640x640 1 clock, 229.5ms\n",
            "image 93/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_9108.jpg: 640x640 1 clock, 210.5ms\n",
            "image 94/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_912.jpg: 640x640 1 clock, 225.0ms\n",
            "image 95/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_9207.jpg: 640x640 1 book, 219.7ms\n",
            "image 96/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_9445.jpg: 640x640 1 laptop, 283.1ms\n",
            "image 97/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_9447.jpg: 640x640 (no detections), 328.1ms\n",
            "image 98/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_9575.jpg: 640x640 (no detections), 316.9ms\n",
            "image 99/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_9691.jpg: 640x640 (no detections), 345.7ms\n",
            "image 100/100 /content/drive/MyDrive/Gray_Size/thyrocare_0_9833.jpg: 640x640 1 clock, 338.1ms\n",
            "Speed: 4.8ms preprocess, 239.3ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/YOLO_Output/Gray_Size_Predictions\u001b[0m\n",
            "Bounding boxes created and saved in /content/drive/MyDrive/YOLO_Output/Gray_Size_Predictions\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a pretrained YOLOv8n model\n",
        "model = YOLO('yolov8n.pt')  # or yolov8n.yaml\n",
        "\n",
        "# Run inference on 'Gray_Size' images\n",
        "results = model.predict(source='/content/drive/MyDrive/Gray_Size', save=True, project='/content/drive/MyDrive/YOLO_Output', name='Gray_Size_Predictions')\n",
        "\n",
        "# Results are saved to /content/drive/MyDrive/YOLO_Output/Gray_Size_Predictions\n",
        "print(\"Bounding boxes created and saved in /content/drive/MyDrive/YOLO_Output/Gray_Size_Predictions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwAj3Y5svKQC"
      },
      "source": [
        "**Extract text from the images with the bounding box and save them in csv file , use the path /content/drive/MyDrive/YOLO_Output/Gray_Size_Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxNz897KDCLG",
        "outputId": "5c92ae5c-7942-42b2-aded-b40119695192"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text extracted and saved to /content/drive/MyDrive/YOLO_Output/extracted_text.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "import pytesseract\n",
        "\n",
        "def extract_text_from_images(yolo_output_folder, csv_output_path):\n",
        "    \"\"\"\n",
        "    Extracts text from images using Tesseract OCR and saves results to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        yolo_output_folder: Path to the folder containing YOLO output images.\n",
        "        csv_output_path: Path to save the output CSV file.\n",
        "    \"\"\"\n",
        "\n",
        "    data = []\n",
        "    for filename in os.listdir(yolo_output_folder):\n",
        "        if filename.endswith(('.jpg', '.png', '.jpeg')):\n",
        "            image_path = os.path.join(yolo_output_folder, filename)\n",
        "            img = cv2.imread(image_path)\n",
        "\n",
        "            # Preprocess the image (e.g., grayscale conversion, noise reduction)\n",
        "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # Perform OCR using pytesseract\n",
        "            extracted_text = pytesseract.image_to_string(gray)\n",
        "\n",
        "            data.append({'filename': filename, 'extracted_text': extracted_text})\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(csv_output_path, index=False)\n",
        "\n",
        "# Example usage:\n",
        "yolo_output_folder = '/content/drive/MyDrive/YOLO_Output/Gray_Size_Predictions'\n",
        "csv_output_path = '/content/drive/MyDrive/YOLO_Output/extracted_text.csv'\n",
        "extract_text_from_images(yolo_output_folder, csv_output_path)\n",
        "print(f\"Text extracted and saved to {csv_output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAs_E8dlz_-z"
      },
      "source": [
        "**use  the path ,  /content/drive/MyDrive/Train_Model/Gray_Size_Boxes to extract text from the gray sized bounding box images and save here in csv file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ordZPmogPbLx",
        "outputId": "432e22d7-87ad-4ee7-e718-f7dce9381843"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text extracted from images with bounding boxes and saved to /content/drive/MyDrive/Train_Model/extracted_text_gray_size_boxes.csv\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import pytesseract\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def extract_text_from_images(image_folder, csv_output_path):\n",
        "    \"\"\"\n",
        "    Extracts text from images using Tesseract OCR and saves results to a CSV file.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for filename in os.listdir(image_folder):\n",
        "        if filename.endswith(('.jpg', '.png', '.jpeg')):\n",
        "            image_path = os.path.join(image_folder, filename)\n",
        "            img = cv2.imread(image_path)\n",
        "\n",
        "            # Preprocess the image (e.g., grayscale conversion, noise reduction)\n",
        "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # Perform OCR using pytesseract\n",
        "            extracted_text = pytesseract.image_to_string(gray)\n",
        "\n",
        "            data.append({'filename': filename, 'extracted_text': extracted_text})\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(csv_output_path, index=False)\n",
        "\n",
        "# Example usage:\n",
        "image_folder_with_boxes = '/content/drive/MyDrive/Train_Model/Gray_Size_Boxes'\n",
        "csv_output_path = '/content/drive/MyDrive/Train_Model/extracted_text_gray_size_boxes.csv'\n",
        "extract_text_from_images(image_folder_with_boxes, csv_output_path)\n",
        "print(f\"Text extracted from images with bounding boxes and saved to {csv_output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6t2GaHjPbZP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L806_gOtPbjC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OURjvSWdPblS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvxiJ5YvwVp5"
      },
      "source": [
        "**Model Training , Train YOLOv3 model using Colab GPU runtime**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjih8HF8nOWn",
        "outputId": "4162b465-f331-4364-db64-6c1a4a678862"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "2/2 [==============================] - 2s 434ms/step - loss: 7.6536e-04 - val_loss: 6.1947e-04\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 0s 85ms/step - loss: 7.4810e-04 - val_loss: 6.1302e-04\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 0s 90ms/step - loss: 7.3443e-04 - val_loss: 6.0904e-04\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 0s 85ms/step - loss: 7.2239e-04 - val_loss: 6.0677e-04\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 0s 93ms/step - loss: 7.1454e-04 - val_loss: 6.0549e-04\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 0s 84ms/step - loss: 7.0659e-04 - val_loss: 6.0476e-04\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 0s 104ms/step - loss: 6.9974e-04 - val_loss: 6.0417e-04\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 6.9432e-04 - val_loss: 6.0361e-04\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 0s 84ms/step - loss: 6.8982e-04 - val_loss: 6.0332e-04\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 0s 91ms/step - loss: 6.8598e-04 - val_loss: 6.0311e-04\n",
            "Training completed. The best model is saved to /content/drive/MyDrive/best_model.h5\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "    # Load the extracted features from the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/extracted_features.csv')\n",
        "\n",
        "    # Feature Engineering: Convert text to numerical vectors (Example using word embeddings)\n",
        "vectorizer = TfidfVectorizer(max_features=1000) # Adjust max_features as needed\n",
        "text_features = vectorizer.fit_transform(df['extracted_text']).toarray()\n",
        "\n",
        "    # Reshape the input data for LSTM (samples, timesteps, features)\n",
        "    # Since we are not using sequential data we set timesteps to 1.\n",
        "text_features = text_features.reshape(text_features.shape[0], 1, text_features.shape[1])\n",
        "\n",
        "    # Define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', input_shape=(text_features.shape[1], text_features.shape[2]))) # Adjust units as needed\n",
        "model.add(Dense(text_features.shape[2])) # Output layer\n",
        "\n",
        "    # Compile the model\n",
        "model.compile(optimizer='adam', loss='mse') # Use appropriate loss function\n",
        "\n",
        "    # Define a checkpoint to save the best model\n",
        "checkpoint = ModelCheckpoint('/content/drive/MyDrive/best_model.h5', monitor='val_loss', save_best_only=True, mode='min')\n",
        "\n",
        "    # Train the model\n",
        "model.fit(text_features, text_features, epochs=10, batch_size=32, validation_split = 0.2, callbacks=[checkpoint])\n",
        "\n",
        "print(\"Training completed. The best model is saved to /content/drive/MyDrive/best_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRHLy2RFrtKM",
        "outputId": "93e61817-3ba8-4396-9dd1-8f0041ad201e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "2/2 [==============================] - 3s 830ms/step - loss: 7.6468e-04 - val_loss: 6.1796e-04\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 0s 81ms/step - loss: 7.4752e-04 - val_loss: 6.1166e-04\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 7.3426e-04 - val_loss: 6.0791e-04\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 0s 171ms/step - loss: 7.2280e-04 - val_loss: 6.0577e-04\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 0s 239ms/step - loss: 7.1373e-04 - val_loss: 6.0484e-04\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 0s 161ms/step - loss: 7.0603e-04 - val_loss: 6.0413e-04\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 0s 205ms/step - loss: 6.9983e-04 - val_loss: 6.0358e-04\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 0s 226ms/step - loss: 6.9476e-04 - val_loss: 6.0322e-04\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 0s 180ms/step - loss: 6.9002e-04 - val_loss: 6.0278e-04\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 0s 236ms/step - loss: 6.8586e-04 - val_loss: 6.0262e-04\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Training completed.\n",
            "📁 Best model saved to: /content/drive/MyDrive/Project-10/best_model.h5\n",
            "📁 SavedModel format also saved to: /content/drive/MyDrive/Project-10/best_model_saved\n",
            "🧠 Vectorizer saved to: /content/drive/MyDrive/Project-10/tfidf_vectorizer.pkl\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# === Load the dataset ===\n",
        "df = pd.read_csv('/content/drive/MyDrive/extracted_features.csv')  # <-- Adjust path if needed\n",
        "texts = df['extracted_text'].fillna('')\n",
        "\n",
        "# === TF-IDF Vectorization ===\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "text_features = vectorizer.fit_transform(texts).toarray()\n",
        "\n",
        "# === Reshape for LSTM input: (samples, timesteps=1, features) ===\n",
        "text_features = text_features.reshape(text_features.shape[0], 1, text_features.shape[1])\n",
        "\n",
        "# === Define the LSTM Autoencoder Model ===\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1, text_features.shape[2])))  # Use Input() layer (avoid old input_shape keyword)\n",
        "model.add(LSTM(50, activation='relu'))\n",
        "model.add(Dense(text_features.shape[2]))  # Output same size as input for reconstruction\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# === Define save paths ===\n",
        "model_h5_path = r'/content/drive/MyDrive/Project-10/best_model.h5'\n",
        "model_saved_path = r'/content/drive/MyDrive/Project-10/best_model_saved'\n",
        "vectorizer_path = r'/content/drive/MyDrive/Project-10/tfidf_vectorizer.pkl'\n",
        "\n",
        "# === Define checkpoint for .h5 ===\n",
        "checkpoint = ModelCheckpoint(model_h5_path, monitor='val_loss', save_best_only=True, mode='min')\n",
        "\n",
        "# === Train the model ===\n",
        "model.fit(\n",
        "    text_features,\n",
        "    text_features,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[checkpoint]\n",
        ")\n",
        "\n",
        "# === Save model also as TensorFlow SavedModel format ===\n",
        "model.save(model_saved_path)\n",
        "\n",
        "# === Save TF-IDF vectorizer ===\n",
        "with open(vectorizer_path, 'wb') as f:\n",
        "    pickle.dump(vectorizer, f)\n",
        "\n",
        "print(\"Training completed.\")\n",
        "print(f\" Best model saved to: {model_h5_path}\")\n",
        "print(f\" SavedModel format also saved to: {model_saved_path}\")\n",
        "print(f\"Vectorizer saved to: {vectorizer_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJwxdY671G0Z"
      },
      "source": [
        "**Validating the Model - Error Check**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4EUaJ_G_dwv",
        "outputId": "a8df8448-765e-4bfc-9039-509d65111816"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3/3 [==============================] - 0s 4ms/step\n",
            "Mean Squared Error: 0.000665780918528501\n",
            "Accuracy: 0.0125\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the best model\n",
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('/content/drive/MyDrive/best_model.h5')\n",
        "\n",
        "# Load the extracted features from the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/extracted_features.csv')\n",
        "\n",
        "# Feature Engineering: Convert text to numerical vectors (Example using word embeddings)\n",
        "vectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed\n",
        "text_features = vectorizer.fit_transform(df['extracted_text']).toarray()\n",
        "\n",
        "# Reshape the input data for LSTM\n",
        "text_features = text_features.reshape(text_features.shape[0], 1, text_features.shape[1])\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(text_features)\n",
        "\n",
        "# Assuming y_true is the same as text_features for autoencoder-like setup\n",
        "y_true = text_features.reshape(text_features.shape[0],text_features.shape[2]) # Reshape to match prediction shape\n",
        "\n",
        "# Calculate accuracy\n",
        "# Accuracy is not the best metric for regression or autoencoder-like tasks.\n",
        "# Consider using Mean Squared Error (MSE) or other relevant metrics.\n",
        "# Convert predictions to class labels if needed.\n",
        "\n",
        "# Example using MSE (Mean Squared Error):\n",
        "mse = np.mean(np.square(y_true - y_pred))\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n",
        "# Example using accuracy (not the best metric for this setup):\n",
        "# Find the indices of the maximum value\n",
        "y_pred_class = np.argmax(y_pred, axis=1)\n",
        "y_true_class = np.argmax(y_true, axis=1)\n",
        "\n",
        "accuracy = accuracy_score(y_true_class, y_pred_class)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZlwy2CkEVGa",
        "outputId": "36693b3b-f77e-4aff-96fc-a8403ad9eed3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "filename           object\n",
            "test_name          object\n",
            "technology         object\n",
            "value              object\n",
            "units              object\n",
            "reference_range    object\n",
            "extracted_text     object\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the extracted features from the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/extracted_features.csv')\n",
        "\n",
        "# Check the data types of all columns\n",
        "print(df.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "we-G4XY3E9Ps",
        "outputId": "db34cd9d-2581-4d61-e89c-489ebbe0c2cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 501ms/step - loss: 7.3501e-04 - val_loss: 7.4208e-04\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 7.1921e-04 - val_loss: 7.3191e-04\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 7.0751e-04 - val_loss: 7.2403e-04\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 0s 102ms/step - loss: 6.9629e-04 - val_loss: 7.1806e-04\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 0s 152ms/step - loss: 6.8879e-04 - val_loss: 7.1297e-04\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 0s 139ms/step - loss: 6.8162e-04 - val_loss: 7.0852e-04\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 0s 126ms/step - loss: 6.7651e-04 - val_loss: 7.0442e-04\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 6.7161e-04 - val_loss: 7.0069e-04\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 0s 133ms/step - loss: 6.6726e-04 - val_loss: 6.9727e-04\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 0s 131ms/step - loss: 6.6412e-04 - val_loss: 6.9423e-04\n",
            "Training completed. The best model is saved to /content/drive/MyDrive/best_model.h5\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# 1. Load and Preprocess Data\n",
        "df = pd.read_csv('/content/drive/MyDrive/extracted_features.csv')\n",
        "\n",
        "# 2. Feature Engineering with TF-IDF (before one-hot encoding)\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "text_features = vectorizer.fit_transform(df['extracted_text']).toarray()\n",
        "text_features = text_features.reshape(text_features.shape[0], 1, text_features.shape[1])\n",
        "\n",
        "# Convert numerical columns to numeric data types\n",
        "numerical_cols = ['value', 'reference_range']\n",
        "for col in numerical_cols:\n",
        "    try:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "    except ValueError:\n",
        "        print(f\"WARNING: Could not convert column '{col}' to numeric.\")\n",
        "\n",
        "# Handle missing values (if any) - choose one method\n",
        "# df.dropna(subset=numerical_cols, inplace=True)  # Remove rows with NaN\n",
        "# df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())  # Impute with mean\n",
        "\n",
        "# One-hot encode string columns (excluding 'extracted_text')\n",
        "string_columns_to_encode = [col for col in df.select_dtypes(include=['object']).columns if col != 'extracted_text']\n",
        "for col in string_columns_to_encode:\n",
        "    df = pd.concat([df, pd.get_dummies(df[col], prefix=col)], axis=1)\n",
        "    df.drop([col], axis=1, inplace=True)\n",
        "\n",
        "# 3. Split Data (including text_features)\n",
        "X_train_text, X_test_text, y_train_text, y_test_text = train_test_split(\n",
        "    text_features, text_features, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# One-hot encoded data (if needed)\n",
        "X_train_ohe, X_test_ohe, y_train_ohe, y_test_ohe = train_test_split(\n",
        "    df, df, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Build and Train LSTM Model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', input_shape=(X_train_text.shape[1], X_train_text.shape[2])))\n",
        "model.add(Dense(X_train_text.shape[2]))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    '/content/drive/MyDrive/best_model.h5', monitor='val_loss', save_best_only=True, mode='min'\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    X_train_text,\n",
        "    y_train_text,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test_text, y_test_text),\n",
        "    callbacks=[checkpoint],\n",
        ")\n",
        "\n",
        "print(\"Training completed. The best model is saved to /content/drive/MyDrive/best_model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHGGkAfr1x7k"
      },
      "source": [
        "**Error and the accuracy of the above model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Q7yfSjKFsAs",
        "outputId": "9a79b5c5-a478-4045-90bb-d361eddecc40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 299ms/step\n",
            "Mean Squared Error: 0.0006932881067909802\n",
            "Accuracy: 0.125\n"
          ]
        }
      ],
      "source": [
        "# Load the best model\n",
        "model = load_model('/content/drive/MyDrive/best_model.h5')\n",
        "\n",
        "# Load the preprocessed data (including one-hot encoded features)\n",
        "df = pd.read_csv('/content/drive/MyDrive/extracted_features.csv')\n",
        "\n",
        "# 2. Feature Engineering with TF-IDF (before one-hot encoding)\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "text_features = vectorizer.fit_transform(df['extracted_text']).toarray()\n",
        "text_features = text_features.reshape(text_features.shape[0], 1, text_features.shape[1])\n",
        "\n",
        "# Convert numerical columns to numeric data types\n",
        "numerical_cols = ['value', 'reference_range']\n",
        "for col in numerical_cols:\n",
        "    try:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "    except ValueError:\n",
        "        print(f\"WARNING: Could not convert column '{col}' to numeric.\")\n",
        "\n",
        "# Handle missing values (if any) - choose one method\n",
        "# df.dropna(subset=numerical_cols, inplace=True)  # Remove rows with NaN\n",
        "# df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())  # Impute with mean\n",
        "\n",
        "# One-hot encode string columns (excluding 'extracted_text')\n",
        "string_columns_to_encode = [col for col in df.select_dtypes(include=['object']).columns if col != 'extracted_text']\n",
        "for col in string_columns_to_encode:\n",
        "    df = pd.concat([df, pd.get_dummies(df[col], prefix=col)], axis=1)\n",
        "    df.drop([col], axis=1, inplace=True)\n",
        "\n",
        "# 3. Split Data (including text_features)\n",
        "X_train_text, X_test_text, y_train_text, y_test_text = train_test_split(\n",
        "    text_features, text_features, test_size=0.2, random_state=42\n",
        ")\n",
        "# ... (rest of your data loading and preprocessing code)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_text)\n",
        "\n",
        "# Calculate accuracy (adjust as needed)\n",
        "mse = np.mean(np.square(y_test_text.reshape(y_test_text.shape[0],y_test_text.shape[2]) - y_pred))\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n",
        "# You can also try to calculate an accuracy like this:\n",
        "# Accuracy is not the best metric for regression or autoencoder-like tasks.\n",
        "# Consider using Mean Squared Error (MSE) or other relevant metrics.\n",
        "y_pred_class = np.argmax(y_pred, axis=1)\n",
        "y_true_class = np.argmax(y_test_text.reshape(y_test_text.shape[0], y_test_text.shape[2]), axis=1)\n",
        "\n",
        "accuracy = accuracy_score(y_true_class, y_pred_class)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAvxaBjY0hqC"
      },
      "source": [
        "**Save the model in pickle file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia1AIi-eGx0b",
        "outputId": "4e329b19-41ab-4f07-f0aa-69aead886144"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to /content/drive/MyDrive/trained_model.pkl\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# Assuming 'model' is your trained LSTM model\n",
        "# Save the model to a pickle file\n",
        "with open('/content/drive/MyDrive/trained_model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "print(\"Model saved to /content/drive/MyDrive/trained_model.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKCqv5w1GyEG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWh2BcL5EURs"
      },
      "source": [
        "**hyper  tune the model to improve accuracy of the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-ZAEDl1JCOiN",
        "outputId": "d593cf96-171f-4f7c-85eb-454ff7f16787"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 2 Complete [00h 00m 02s]\n",
            "\n",
            "Best val_loss So Far: None\n",
            "Total elapsed time: 00h 00m 04s\n",
            "\n",
            "Search: Running Trial #3\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "128               |96                |units\n",
            "sgd               |adam              |optimizer\n",
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n",
            "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n",
            "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n",
            "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n",
            "    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n",
            "    return model.fit(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\", line 58, in quick_execute\n",
            "    except TypeError as e:\n",
            "tensorflow.python.framework.errors_impl.UnimplementedError: Graph execution error:\n",
            "\n",
            "Detected at node 'sequential/Cast' defined at (most recent call last):\n",
            "    File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "    File \"<frozen runpy>\", line 88, in _run_code\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "      ColabKernelApp.launch_instance()\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "      app.start()\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "      self.io_loop.start()\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "      self.asyncio_loop.run_forever()\n",
            "    File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "      self._run_once()\n",
            "    File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "      handle._run()\n",
            "    File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "      self._context.run(self._callback, *self._args)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "      await self.process_one()\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "      await dispatch(*args)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "      await result\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "      reply_content = await reply_content\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "      res = shell.run_cell(\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "      return super().run_cell(*args, **kwargs)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "      result = self._run_cell(\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "      return runner(coro)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "      coro.send(None)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "      if (await self.run_code(code, result,  async_=asy)):\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "      exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "    File \"<ipython-input-15-d8fc0605bf33>\", line 32, in <cell line: 0>\n",
            "      tuner.search(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 234, in search\n",
            "      self._try_run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n",
            "      self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n",
            "      results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n",
            "      obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n",
            "      results = self.hypermodel.fit(hp, model, *args, **kwargs)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n",
            "      return model.fit(*args, **kwargs)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n",
            "      return fn(*args, **kwargs)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 1685, in fit\n",
            "      tmp_logs = self.train_function(iterator)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 1284, in train_function\n",
            "      return step_function(self, iterator)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 1268, in step_function\n",
            "      outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 1249, in run_step\n",
            "      outputs = model.train_step(data)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 1050, in train_step\n",
            "      y_pred = self(x, training=True)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n",
            "      return fn(*args, **kwargs)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 558, in __call__\n",
            "      return super().__call__(*args, **kwargs)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n",
            "      return fn(*args, **kwargs)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/base_layer.py\", line 1145, in __call__\n",
            "      outputs = call_fn(inputs, *args, **kwargs)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n",
            "      return fn(*args, **kwargs)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/sequential.py\", line 412, in call\n",
            "      return super().call(inputs, training=training, mask=mask)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/functional.py\", line 512, in call\n",
            "      return self._run_internal_graph(inputs, training=training, mask=mask)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/functional.py\", line 651, in _run_internal_graph\n",
            "      y = self._conform_to_reference_input(y, ref_input=x)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/functional.py\", line 748, in _conform_to_reference_input\n",
            "      tensor = tf.cast(tensor, dtype=ref_input.dtype)\n",
            "Node: 'sequential/Cast'\n",
            "Cast string to float is not supported\n",
            "\t [[{{node sequential/Cast}}]] [Op:__inference_train_function_10638]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Number of consecutive failures exceeded the limit of 3.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n    return model.fit(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\", line 58, in quick_execute\n    except TypeError as e:\ntensorflow.python.framework.errors_impl.UnimplementedError: Graph execution error:\n\nDetected at node 'sequential/Cast' defined at (most recent call last):\n    File \"<frozen runpy>\", line 198, in _run_module_as_main\n    File \"<frozen runpy>\", line 88, in _run_code\n    File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n      ColabKernelApp.launch_instance()\n    File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-15-d8fc0605bf33>\", line 32, in <cell line: 0>\n      tuner.search(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n    File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 234, in search\n      self._try_run_and_update_trial(trial, *fit_args, **fit_kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n      self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n      results = self.run_trial(trial, *fit_args, **fit_kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n      obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n      results = self.hypermodel.fit(hp, model, *args, **kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n      return model.fit(*args, **kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 1050, in train_step\n      y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/sequential.py\", line 412, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/functional.py\", line 651, in _run_internal_graph\n      y = self._conform_to_reference_input(y, ref_input=x)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/functional.py\", line 748, in _conform_to_reference_input\n      tensor = tf.cast(tensor, dtype=ref_input.dtype)\nNode: 'sequential/Cast'\nCast string to float is not supported\n\t [[{{node sequential/Cast}}]] [Op:__inference_train_function_10638]\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-d8fc0605bf33>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Perform hyperparameter search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Get the best hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36mon_trial_end\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTrial\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/oracle.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mLOCKS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mTHREADS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthread_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mneed_acquire\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mTHREADS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/oracle.py\u001b[0m in \u001b[0;36mend_trial\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_order\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_consecutive_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/oracle.py\u001b[0m in \u001b[0;36m_check_consecutive_failures\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    543\u001b[0m                 \u001b[0mconsecutive_failures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconsecutive_failures\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_consecutive_failed_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m                 raise RuntimeError(\n\u001b[0m\u001b[1;32m    546\u001b[0m                     \u001b[0;34m\"Number of consecutive failures exceeded the limit \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m                     \u001b[0;34mf\"of {self.max_consecutive_failed_trials}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Number of consecutive failures exceeded the limit of 3.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n    return model.fit(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\", line 58, in quick_execute\n    except TypeError as e:\ntensorflow.python.framework.errors_impl.UnimplementedError: Graph execution error:\n\nDetected at node 'sequential/Cast' defined at (most recent call last):\n    File \"<frozen runpy>\", line 198, in _run_module_as_main\n    File \"<frozen runpy>\", line 88, in _run_code\n    File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n      ColabKernelApp.launch_instance()\n    File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-15-d8fc0605bf33>\", line 32, in <cell line: 0>\n      tuner.search(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n    File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 234, in search\n      self._try_run_and_update_trial(trial, *fit_args, **fit_kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n      self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n      results = self.run_trial(trial, *fit_args, **fit_kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n      obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n      results = self.hypermodel.fit(hp, model, *args, **kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n      return model.fit(*args, **kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 1050, in train_step\n      y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/sequential.py\", line 412, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/functional.py\", line 651, in _run_internal_graph\n      y = self._conform_to_reference_input(y, ref_input=x)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/functional.py\", line 748, in _conform_to_reference_input\n      tensor = tf.cast(tensor, dtype=ref_input.dtype)\nNode: 'sequential/Cast'\nCast string to float is not supported\n\t [[{{node sequential/Cast}}]] [Op:__inference_train_function_10638]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install -q -U keras-tuner\n",
        "import kerastuner as kt\n",
        "from kerastuner.tuners import RandomSearch\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=hp.Int('units', min_value=32, max_value=128, step=32),\n",
        "                   activation='relu',\n",
        "                   input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Dense(1))  # Output layer\n",
        "\n",
        "    model.compile(optimizer=hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop']),\n",
        "                  loss='mse',  # Use appropriate loss function\n",
        "                  metrics=['mse']) # Add mse as metric for monitoring\n",
        "\n",
        "    return model\n",
        "\n",
        "# Initialize the tuner\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_loss',  # or 'val_mse' if you use val_mse as metric\n",
        "    max_trials=5,  # Number of hyperparameter combinations to try\n",
        "    executions_per_trial=3,  # Number of times to run each trial\n",
        "    directory='my_dir',\n",
        "    project_name='helloworld'\n",
        ")\n",
        "\n",
        "# Perform hyperparameter search\n",
        "tuner.search(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"Best hyperparameters: {best_hps.values}\")\n",
        "\n",
        "# Build and train the best model\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "best_model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
        "\n",
        "\n",
        "# Evaluate the best model\n",
        "loss, mse = best_model.evaluate(X_test, y_test)\n",
        "print(\"Loss:\", loss)\n",
        "print(\"MSE:\", mse)\n",
        "\n",
        "# Save the best model\n",
        "best_model.save('/content/drive/MyDrive/best_tuned_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1POaJyNLnOdY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN40wmVYhlJ_"
      },
      "source": [
        "Build a Custom OCR by combining YOLO and Tesseract, to read the specific contents of a Lab\n",
        "\n",
        "Report and convert it into an editable file. Use  YOLO_V3 to trained on the personal dataset.\n",
        "\n",
        "Then the coordinates of the detected objects are passed for cropping the detected objects and\n",
        "\n",
        "storing them in another list. This list is passed through the Tesseract to get the desired output.\n",
        "\n",
        "**Model**\n",
        "\n",
        "● You can train a custom YOLO_V3 model using your custom dataset.\n",
        "\n",
        "● Make a folder named model and put the weights file inside it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbkAiuQDw9o8"
      },
      "outputs": [],
      "source": [
        "#Revised One"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhAvR62ZnKjI"
      },
      "outputs": [],
      "source": [
        "Dataset_OCR/\n",
        "├── images/\n",
        "│   ├── train/\n",
        "│   │   └── your_training_images.jpg  # (and other images)\n",
        "│   └── val/\n",
        "│       └── your_validation_images.jpg  # (and other images)\n",
        "└── labels/\n",
        "    ├── train/\n",
        "    │   └── your_training_labels.txt  # (and other labels)\n",
        "    └── val/\n",
        "        └── your_validation_labels.txt  # (and other labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0xzJmZvwoTg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTv7klgIvhPr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_21Xd5suvhTo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxhozWo9vhXm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIHDR_-_vhbP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIsc5q8lvhdM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTA_AtoDvhgG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
